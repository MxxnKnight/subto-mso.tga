# Agent Instructions for Subtitle Search Bot

> **CAUTION:** This file contains critical information for the operation and maintenance of this bot. **NEVER DELETE THIS FILE.**

## 1. Project Overview

This is a Telegram bot that scrapes movie and series subtitle information from `malayalamsubtitles.org`, stores it in a **PostgreSQL database**, and allows users to search for and download subtitle files. The project has been refactored to use a persistent database instead of JSON files to prevent data loss and service restarts.

### Core Dependencies
- `fastapi` & `uvicorn`: For the web server.
- `aiohttp`: For asynchronous HTTP requests.
- `asyncpg` & `psycopg2-binary`: For connecting to the PostgreSQL database.
- `requests` & `beautifulsoup4`: For web scraping.

## 2. Core Architecture

The project uses a decoupled architecture for stability and scalability.

-   **Web Service (Render):** The live Telegram bot is a FastAPI application defined in `app.py`. It is hosted on Render's free web service tier. Its primary role is to respond to user interactions by querying the central PostgreSQL database.

-   **Scraper (GitHub Actions):** Data collection is handled by `scraper.py`. This script is executed automatically once a day by a GitHub Actions workflow in `.github/workflows/scraper.yml`. The workflow connects to the PostgreSQL database, scrapes the latest subtitles, and directly inserts or updates records in the database. **It no longer commits any files to the repository.** This architecture prevents the bot from restarting every time the data is updated.

## 3. Key File Details & Logic

-   **`app.py`:** The main FastAPI application.
    -   **Database Connection:** On startup, `init_db` connects to the PostgreSQL database using `asyncpg` and creates the `users` and `subtitles` tables if they don't exist.
    -   **Data Handling:** All functions that previously accessed in-memory JSON files (e.g., `search_content`, `handle_callback_query`) have been rewritten to be `async` and perform direct SQL queries against the database.
    -   **Admin Commands:** Commands like `/stats` and `/delete` now operate directly on the database.

-   **`scraper.py`:** The data scraper.
    -   **Database Logic:** The `main` function is `async` and connects to the PostgreSQL database. It uses an "UPSERT" (`INSERT ... ON CONFLICT DO UPDATE`) command to add new entries and update existing ones.
    -   **No File I/O:** The script no longer reads from or writes to any JSON files.

-   `.github/workflows/scraper.yml`: The GitHub Actions configuration.
    -   It is now configured to simply run the `scraper.py` script.
    -   The `DATABASE_URL` is passed as a secret environment variable.
    -   **The `git-auto-commit-action` has been removed.**

## 4. Development & Testing

-   **Testing the Scraper:** The scraper is the most complex part. Test locally before committing changes.
    -   `pip install -r requirements.txt`
    -   You must have a local PostgreSQL instance or a cloud-hosted one.
    -   Set the `DATABASE_URL` environment variable.
    -   `SCRAPER_MAX_PAGES=1 python scraper.py`
-   **Key Challenge - Scraper Fragility:** The scraper's logic is tightly coupled to the HTML structure of `malayalamsubtitles.org`. **If the website's HTML, class names, or IDs change, the scraper will break.**

## 5. Configuration

-   **Environment Variables:**
    -   `DATABASE_URL`: **This is now the most critical variable.** It must be the connection string for your PostgreSQL database and must be set as a secret in both Render and the GitHub repository settings for the Action to work.
    -   `TELEGRAM_BOT_TOKEN`: The secret token for your bot.
    -   `OWNER_ID`: Your personal Telegram User ID for admin commands.
    -   `WEBHOOK_SECRET`: **Auto-generated by Render.** You do not need to set this.
    -   `SCRAPER_MAX_PAGES`: (For GitHub Action) Controls the maximum number of pages the scraper will process. Default is `6`.

## 6. Bot Functioning

The user-facing functionality of the bot remains the same. It still provides menus for navigation and allows users to search for, view details of, and download subtitles for movies and series. The backend change to a persistent database makes the service more reliable.
